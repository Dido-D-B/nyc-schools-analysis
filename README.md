# NYC School Analysis - Onboarding Week Project

ğŸ—“ï¸ Project duration: 1 week

From Google Sheet analysis â†’ Python data cleaning â†’ SQL database population

## Objective

This onboarding project focused on understanding the **NYC public schools dataset** through different stages of the data workflow â€” from exploratory analysis to database integration.

The main goal was to:

* Explore and analyze data from various sources
* Clean and normalize data for consistency
* Load the prepared data into a PostgreSQL database
* Practice SQL querying, documentation, and version control in VS Code and GitHub

## Tools & Technologies

* **Google Sheets**: Initial data exploration and cleaning
* **Python** (Pandas, SQLAlchemy): Data wrangling, cleaning, and export to DB
* **PostgreSQL**: Database storage and querying
* **VS Code** + **GitHub**: Version control and project organization
* **Matplotlib** / **Seaborn**: Data visualization 
* **SQL**: Querying and aggregating cleaned data

## Project Structure 

````
NYC-SCHOOLS-ANALYSIS/
â”‚
â”œâ”€â”€ incident_analysis_google_sheets/                        # Initial analysis of incident reports in Google Sheets
â”‚   â”œâ”€â”€ raw_data.csv
â”‚   â”œâ”€â”€ cleaned_data_google_sheets.csv
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ school_directory_exploration_python/                    # Python-based data exploration and visualization
â”‚   â”œâ”€â”€ notebook.ipynb
â”‚   â”œâ”€â”€ visuals/
â”‚   â”‚   â”œâ”€â”€ avg_num_students_borough.png
â”‚   â”‚   â”œâ”€â”€ dist_school_size_borough.png
â”‚   â”‚   â”œâ”€â”€ num_schools_borough.png
â”‚   â”‚   â””â”€â”€ start_grade_dist_borough.png
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ database_population/                                    # Data cleaning and database upload (PostgreSQL)
â”‚   â”œâ”€â”€ data_cleaning.ipynb
â”‚   â”œâ”€â”€ cleaned_sat_results.csv
â”‚   â””â”€â”€  README.md
â”‚
â”œâ”€â”€ database_queries_sql/                                   # SQL queries and analysis examples
â”‚   â”œâ”€â”€ queries.ipynb
â”‚   â””â”€â”€  README.md
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
````

## Data Cleaning Summary

The SAT results dataset was cleaned and prepared before being uploaded to the database.
Key steps included:

* **Normalization of column names**
  â†’ lowercase, underscores, and consistent naming conventions
* **Removal of duplicate rows and columns**
  â†’ handled identical SAT score columns and repeated school entries
* **Data type conversions**
  â†’ converted object/string columns to numeric where appropriate
* **Missing data handling**
  â†’ instead of dropping, imputed missing values (pct_students_tested and academic_tier_rating) using the mean
* **Validation checks**
  â†’ ensured SAT scores fell within valid ranges (200â€“800)
  â†’ confirmed consistency of percentage values and IDs

## Database Population

After cleaning, the data was uploaded into a PostgreSQL database (nyc_schools schema).

**Table name**: dido_sat_results

<table>
  <thead>
    <tr>
      <th>Column</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr><td><code>dbn</code></td><td><code>TEXT</code></td><td>unique school identifier (primary key)</td></tr>
    <tr><td><code>school_name</code></td><td><code>TEXT</code></td><td>name of the school</td></tr>
    <tr><td><code>num_of_sat_test_takers</code></td><td><code>INTEGER</code></td><td>number of students taking SAT</td></tr>
    <tr><td><code>sat_critical_reading_avg_score</code></td><td><code>INTEGER</code></td><td>average reading score</td></tr>
    <tr><td><code>sat_math_avg_score</code></td><td><code>INTEGER</code></td><td>average math score</td></tr>
    <tr><td><code>sat_writing_avg_score</code></td><td><code>INTEGER</code></td><td>average writing score</td></tr>
    <tr><td><code>pct_students_tested</code></td><td><code>DOUBLE PRECISION</code></td><td>proportion of students tested (0â€“1)</td></tr>
    <tr><td><code>academic_tier_rating</code></td><td><code>DOUBLE PRECISION</code></td><td>school tier rating</td></tr>
    <tr><td><code>internal_school_id</code></td><td><code>BIGINT</code></td><td>internal identifier</td></tr>
  </tbody>
</table>

## Key Learnings

* Practiced a full data pipeline from raw data to database integration
* Gained experience with ETL workflows in Python and SQL
* Improved understanding of data cleaning best practices and when to impute vs. drop missing values
* Applied version control (Git) to manage notebook versions and documentation
 


